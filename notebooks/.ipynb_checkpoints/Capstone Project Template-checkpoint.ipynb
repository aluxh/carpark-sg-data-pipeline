{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parking Lots Availability and Forecasting App\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The goal of this project is to build data pipeline for gathering real-time carpark lots availability and weather datasets from Data.gov.sg. These data are extracted via API, and stored them in the S3 bucket before ingesting them into the Dare Warehouse. These data will be used to power the mechanics of the Parking Lots Availability and Forecasting App\n",
    "\n",
    "The objectives are to:\n",
    "\n",
    "1. Building an ETL pipeline using Apache Airflow to extract data via API and store them in AWS S3.\n",
    "2. Ingesting data from AWS S3, and staging them in Redshift, and\n",
    "3. Transforming data into a set of dimensional and fact tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import configparser\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "import logging\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import io\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sql extension is already loaded. To reload it, use:\n",
      "  %reload_ext sql\n"
     ]
    }
   ],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Scope and Datasets\n",
    "\n",
    "#### Project Scope\n",
    "\n",
    "I am always curious about whether with weather data, I could forecast the carpark availability in specific locations in the next hours or days. So, this project that I am doing it to first build data pipeline to collect past and live weather and carpark availability data with 10-minutes interval. Then, store these data in Redshift.\n",
    "\n",
    "Although this part of the project is outside of capstone project scope, the plan is to use the data to run exploration and train machine learning models. Then, use the data and models to power Web Apps that runs on R Shiny App.\n",
    "\n",
    "##### Tools\n",
    "\n",
    "I'm using Docker containers to build the data pipeline, as they are convenient to deploy technology stack without going through the hassle of installing. Furthermore, at the end of it, I could push the containers and host them in AWS services.\n",
    "\n",
    "The tech-stack that I'm using for developing the pipeline are:\n",
    "\n",
    "- __PostgreSQL:__ Deployed 2 instances. One is meant for Airflow metadata DB, while the other is used for initial development of the Data Warehouse locally. One of the instance can be removed after migrated to Redshift.\n",
    "- __Pgadmin:__ DB Administrative Tool\n",
    "- __Jyupter Notebook:__ Using jyupter notebook to develop environment for automating Redshift Data Warehouse deployment, Developing and Testing ETL codes, and Run simple data exploration.\n",
    "- __Airflow:__ Using airflow, with local executors to design and deploy codes as workflows.\n",
    "- __AWS S3:__ Using S3 for storing the data after quering the APIs.\n",
    "- __AWS Redshift:__ Using AWS Redshift for Data Warehouse\n",
    "\n",
    "\n",
    "#### Dataset Used\n",
    "The following are datasets used in the project. They are all extracted from data.gov.sg using API calls:\n",
    "- Temperature events: temperature measurement taken every 10 minutes interval.\n",
    "- Rainfall events: Rainfall measurement taken every 10 minutes interval.\n",
    "- Carpark Availability: Carpark availability count taken every 10 minutes interval.\n",
    "- Carpark Information: Dataset that shows the carpark number and locations.\n",
    "- Weather Stations Information: Dataset that show the weather stations number and their locations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Parameters for connecting to redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "\n",
    "DWH_DB                 = config.get(\"DWH\",\"DWH_DB\")\n",
    "DWH_DB_USER            = config.get(\"DWH\",\"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD        = config.get(\"DWH\",\"DWH_DB_PASSWORD\")\n",
    "DWH_PORT               = config.get(\"DWH\",\"DWH_PORT\")\n",
    "\n",
    "DWH_ENDPOINT           = config.get(\"CLUSTER\", \"HOST\")\n",
    "DWH_ROLE_ARN           = config.get(\"IAM_ROLE\", \"ARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Connect to Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql://awsuser:passw0rD@dwhcluster.crttik8cimnv.us-west-2.redshift.amazonaws.com:5439/dev\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Connected: awsuser@dev'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DWH_DB_USER, DWH_DB_PASSWORD, DWH_ENDPOINT, DWH_PORT,DWH_DB)\n",
    "print(conn_string)\n",
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://awsuser:***@dwhcluster.crttik8cimnv.us-west-2.redshift.amazonaws.com:5439/dev\n",
      "3 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>date_time</th>\n",
       "        <th>station_id</th>\n",
       "        <th>temperature</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2019-08-25 01:40:00+00:00</td>\n",
       "        <td>S100</td>\n",
       "        <td>27.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2019-08-25 01:40:00+00:00</td>\n",
       "        <td>S104</td>\n",
       "        <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2019-08-25 01:40:00+00:00</td>\n",
       "        <td>S106</td>\n",
       "        <td>26.4</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(datetime.datetime(2019, 8, 25, 1, 40, tzinfo=psycopg2.tz.FixedOffsetTimezone(offset=0, name=None)), 'S100', 27.7),\n",
       " (datetime.datetime(2019, 8, 25, 1, 40, tzinfo=psycopg2.tz.FixedOffsetTimezone(offset=0, name=None)), 'S104', 28.0),\n",
       " (datetime.datetime(2019, 8, 25, 1, 40, tzinfo=psycopg2.tz.FixedOffsetTimezone(offset=0, name=None)), 'S106', 26.4)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql SELECT * FROM temperature_events LIMIT 3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://awsuser:***@dwhcluster.crttik8cimnv.us-west-2.redshift.amazonaws.com:5439/dev\n",
      "3 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>date_time</th>\n",
       "        <th>station_id</th>\n",
       "        <th>rainfall</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2019-08-25 01:50:00+00:00</td>\n",
       "        <td>S07</td>\n",
       "        <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2019-08-25 01:50:00+00:00</td>\n",
       "        <td>S08</td>\n",
       "        <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2019-08-25 01:50:00+00:00</td>\n",
       "        <td>S100</td>\n",
       "        <td>0.0</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(datetime.datetime(2019, 8, 25, 1, 50, tzinfo=psycopg2.tz.FixedOffsetTimezone(offset=0, name=None)), 'S07', 0.0),\n",
       " (datetime.datetime(2019, 8, 25, 1, 50, tzinfo=psycopg2.tz.FixedOffsetTimezone(offset=0, name=None)), 'S08', 0.0),\n",
       " (datetime.datetime(2019, 8, 25, 1, 50, tzinfo=psycopg2.tz.FixedOffsetTimezone(offset=0, name=None)), 'S100', 0.0)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql SELECT * FROM rainfall_events LIMIT 3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://awsuser:***@dwhcluster.crttik8cimnv.us-west-2.redshift.amazonaws.com:5439/dev\n",
      "3 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>date_time</th>\n",
       "        <th>carpark_id</th>\n",
       "        <th>lots_available</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2019-08-25 01:50:00+00:00</td>\n",
       "        <td>A10</td>\n",
       "        <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2019-08-25 01:50:00+00:00</td>\n",
       "        <td>A100</td>\n",
       "        <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2019-08-25 01:50:00+00:00</td>\n",
       "        <td>A11</td>\n",
       "        <td>155</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(datetime.datetime(2019, 8, 25, 1, 50, tzinfo=psycopg2.tz.FixedOffsetTimezone(offset=0, name=None)), 'A10', 30),\n",
       " (datetime.datetime(2019, 8, 25, 1, 50, tzinfo=psycopg2.tz.FixedOffsetTimezone(offset=0, name=None)), 'A100', 7),\n",
       " (datetime.datetime(2019, 8, 25, 1, 50, tzinfo=psycopg2.tz.FixedOffsetTimezone(offset=0, name=None)), 'A11', 155)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql SELECT * FROM carpark_availability LIMIT 3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "\n",
    "\n",
    "\n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing cleaning tasks here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
